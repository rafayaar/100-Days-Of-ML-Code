{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1i4T5eSnXc6",
        "outputId": "01bce6ab-2ff1-4940-cca7-21b7664454fc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 0, Total Reward: 20.0\n",
            "Episode: 10, Total Reward: 34.0\n",
            "Episode: 20, Total Reward: 51.0\n",
            "Episode: 30, Total Reward: 39.0\n",
            "Episode: 40, Total Reward: 85.0\n",
            "Episode: 50, Total Reward: 82.0\n",
            "Episode: 60, Total Reward: 198.0\n",
            "Episode: 70, Total Reward: 484.0\n",
            "Episode: 80, Total Reward: 99.0\n",
            "Episode: 90, Total Reward: 118.0\n",
            "Episode: 100, Total Reward: 40.0\n",
            "Episode: 110, Total Reward: 64.0\n",
            "Episode: 120, Total Reward: 67.0\n",
            "Episode: 130, Total Reward: 97.0\n",
            "Episode: 140, Total Reward: 269.0\n",
            "Episode: 150, Total Reward: 130.0\n",
            "Episode: 160, Total Reward: 77.0\n",
            "Episode: 170, Total Reward: 62.0\n",
            "Episode: 180, Total Reward: 48.0\n",
            "Episode: 190, Total Reward: 80.0\n",
            "Episode: 200, Total Reward: 39.0\n",
            "Episode: 210, Total Reward: 48.0\n",
            "Episode: 220, Total Reward: 75.0\n",
            "Episode: 230, Total Reward: 93.0\n",
            "Episode: 240, Total Reward: 118.0\n",
            "Episode: 250, Total Reward: 500.0\n",
            "Episode: 260, Total Reward: 314.0\n",
            "Episode: 270, Total Reward: 136.0\n",
            "Episode: 280, Total Reward: 103.0\n",
            "Episode: 290, Total Reward: 98.0\n",
            "Episode: 300, Total Reward: 169.0\n",
            "Episode: 310, Total Reward: 273.0\n",
            "Episode: 320, Total Reward: 412.0\n",
            "Episode: 330, Total Reward: 253.0\n",
            "Episode: 340, Total Reward: 280.0\n",
            "Episode: 350, Total Reward: 471.0\n",
            "Episode: 360, Total Reward: 500.0\n",
            "Episode: 370, Total Reward: 500.0\n",
            "Episode: 380, Total Reward: 500.0\n",
            "Episode: 390, Total Reward: 359.0\n",
            "Episode: 400, Total Reward: 339.0\n",
            "Episode: 410, Total Reward: 500.0\n",
            "Episode: 420, Total Reward: 500.0\n",
            "Episode: 430, Total Reward: 500.0\n",
            "Episode: 440, Total Reward: 500.0\n",
            "Episode: 450, Total Reward: 500.0\n",
            "Episode: 460, Total Reward: 65.0\n",
            "Episode: 470, Total Reward: 14.0\n",
            "Episode: 480, Total Reward: 500.0\n",
            "Episode: 490, Total Reward: 9.0\n",
            "Episode: 500, Total Reward: 8.0\n",
            "Episode: 510, Total Reward: 10.0\n",
            "Episode: 520, Total Reward: 9.0\n",
            "Episode: 530, Total Reward: 12.0\n",
            "Episode: 540, Total Reward: 14.0\n",
            "Episode: 550, Total Reward: 155.0\n",
            "Episode: 560, Total Reward: 185.0\n",
            "Episode: 570, Total Reward: 138.0\n",
            "Episode: 580, Total Reward: 175.0\n",
            "Episode: 590, Total Reward: 156.0\n",
            "Episode: 600, Total Reward: 141.0\n",
            "Episode: 610, Total Reward: 186.0\n",
            "Episode: 620, Total Reward: 180.0\n",
            "Episode: 630, Total Reward: 134.0\n",
            "Episode: 640, Total Reward: 204.0\n",
            "Episode: 650, Total Reward: 362.0\n",
            "Episode: 660, Total Reward: 348.0\n",
            "Episode: 670, Total Reward: 269.0\n",
            "Episode: 680, Total Reward: 500.0\n",
            "Episode: 690, Total Reward: 241.0\n",
            "Episode: 700, Total Reward: 101.0\n",
            "Episode: 710, Total Reward: 103.0\n",
            "Episode: 720, Total Reward: 87.0\n",
            "Episode: 730, Total Reward: 54.0\n",
            "Episode: 740, Total Reward: 51.0\n",
            "Episode: 750, Total Reward: 120.0\n",
            "Episode: 760, Total Reward: 77.0\n",
            "Episode: 770, Total Reward: 93.0\n",
            "Episode: 780, Total Reward: 113.0\n",
            "Episode: 790, Total Reward: 70.0\n",
            "Episode: 800, Total Reward: 58.0\n",
            "Episode: 810, Total Reward: 116.0\n",
            "Episode: 820, Total Reward: 68.0\n",
            "Episode: 830, Total Reward: 77.0\n",
            "Episode: 840, Total Reward: 98.0\n",
            "Episode: 850, Total Reward: 98.0\n",
            "Episode: 860, Total Reward: 113.0\n",
            "Episode: 870, Total Reward: 88.0\n",
            "Episode: 880, Total Reward: 118.0\n",
            "Episode: 890, Total Reward: 198.0\n",
            "Episode: 900, Total Reward: 44.0\n",
            "Episode: 910, Total Reward: 105.0\n",
            "Episode: 920, Total Reward: 75.0\n",
            "Episode: 930, Total Reward: 161.0\n",
            "Episode: 940, Total Reward: 127.0\n",
            "Episode: 950, Total Reward: 81.0\n",
            "Episode: 960, Total Reward: 93.0\n",
            "Episode: 970, Total Reward: 500.0\n",
            "Episode: 980, Total Reward: 344.0\n",
            "Episode: 990, Total Reward: 500.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "env = gym.make('CartPole-v1')  \n",
        "\n",
        "num_episodes = 1000\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    episode_states, episode_actions, episode_rewards = [], [], []\n",
        "\n",
        "class PolicyNetwork(tf.keras.Model):\n",
        "    def __init__(self, num_actions):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
        "        self.dense2 = tf.keras.layers.Dense(num_actions, activation='softmax')\n",
        "\n",
        "    def call(self, state):\n",
        "        x = self.dense1(state)\n",
        "        return self.dense2(x)\n",
        "\n",
        "def compute_loss(logits, actions, advantages):\n",
        "    action_masks = tf.one_hot(actions, num_actions)\n",
        "    log_prob = tf.math.log(tf.reduce_sum(action_masks * tf.nn.softmax(logits), axis=1))\n",
        "    return -tf.reduce_sum(log_prob * advantages)\n",
        "\n",
        "num_actions = 2\n",
        "learning_rate = 0.01\n",
        "gamma = 0.99\n",
        "\n",
        "policy_network = PolicyNetwork(num_actions)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
        "\n",
        "num_episodes = 1000\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    episode_states, episode_actions, episode_rewards = [], [], []\n",
        "\n",
        "    while True:\n",
        "        logits = policy_network(np.expand_dims(state, axis=0))\n",
        "        action = np.random.choice(num_actions, p=np.squeeze(logits))\n",
        "\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        episode_states.append(state)\n",
        "        episode_actions.append(action)\n",
        "        episode_rewards.append(reward)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "    discounted_rewards = []\n",
        "    running_add = 0\n",
        "    for r in reversed(episode_rewards):\n",
        "        running_add = running_add * gamma + r\n",
        "        discounted_rewards.insert(0, running_add)\n",
        "\n",
        "    mean_reward = np.mean(discounted_rewards)\n",
        "    std_reward = np.std(discounted_rewards)\n",
        "    normalized_rewards = (discounted_rewards - mean_reward) / (std_reward + 1e-8)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = policy_network(tf.convert_to_tensor(episode_states, dtype=tf.float32))\n",
        "        loss = compute_loss(logits, episode_actions, normalized_rewards)\n",
        "\n",
        "    grads = tape.gradient(loss, policy_network.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, policy_network.trainable_variables))\n",
        "\n",
        "    if episode % 10 == 0:\n",
        "        print(f\"Episode: {episode}, Total Reward: {sum(episode_rewards)}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
