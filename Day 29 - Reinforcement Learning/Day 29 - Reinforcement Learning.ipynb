{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9008a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Q-table:\n",
      "[[[-1.          0.         -1.          0.        ]\n",
      "  [-1.         -1.         -1.9         0.        ]\n",
      "  [-1.         -1.         -1.9         0.        ]\n",
      "  [-1.         -1.         -1.9         0.        ]\n",
      "  [-1.         -1.         -1.9         0.        ]\n",
      "  [-1.         -1.         -1.9         0.        ]\n",
      "  [-1.         -1.         -1.9         0.        ]\n",
      "  [-1.         -1.         -1.9         0.        ]\n",
      "  [-1.         -1.         -1.9         0.        ]\n",
      "  [ 0.         -1.         -1.          0.        ]]\n",
      "\n",
      " [[-1.9         0.         -1.         -1.        ]\n",
      "  [-1.9        -1.         -1.9        -1.        ]\n",
      "  [-1.89997902 -1.89999187 -2.70973499 -1.        ]\n",
      "  [-1.89994054 -1.89993444 -2.70933692 -1.        ]\n",
      "  [-1.89974532 -1.89964084 -2.69844015 -1.        ]\n",
      "  [-1.89958194 -1.89894334 -2.66872149 -1.        ]\n",
      "  [-1.89916815 -1.88911934 -2.69232761 -1.        ]\n",
      "  [-1.89002773 -1.88793231 -2.59250279 -1.        ]\n",
      "  [-1.         -1.89579923 -1.89828    -1.        ]\n",
      "  [ 0.         -1.9        -1.         -1.        ]]\n",
      "\n",
      " [[-1.9         0.         -1.         -1.        ]\n",
      "  [-2.70999995 -1.         -1.9        -1.9       ]\n",
      "  [-2.28735325 -1.89999998 -2.41148362 -1.89999998]\n",
      "  [-2.08488284 -2.08379155 -2.26705082 -1.89993901]\n",
      "  [-2.0316093  -2.04028309 -2.01125832 -1.89837764]\n",
      "  [-1.97004227 -1.96884091 -2.07825773 -1.89272967]\n",
      "  [-1.95102652 -1.95291441 -2.0634065  -1.89750957]\n",
      "  [-1.86402711 -1.8889722  -1.98119264 -1.86235572]\n",
      "  [-1.         -2.40501808 -1.8038754  -1.86011189]\n",
      "  [ 0.         -1.9        -1.         -1.        ]]\n",
      "\n",
      " [[-1.9         0.         -1.         -1.        ]\n",
      "  [-2.70999999 -1.         -1.9        -1.9       ]\n",
      "  [-2.27944813 -1.9        -1.9        -2.33830704]\n",
      "  [-2.01194096 -2.03610815 -1.89951358 -1.99813068]\n",
      "  [-1.95154784 -1.98221985 -1.89757106 -1.93576262]\n",
      "  [-1.89755993 -1.89878198 -1.89040956 -1.94711597]\n",
      "  [-1.89800612 -2.01570011 -1.85894918 -1.99168549]\n",
      "  [-1.78366846 -1.82113526 -1.77433131 -1.86676812]\n",
      "  [-1.         -2.15256894 -1.         -1.55001183]\n",
      "  [ 0.         -1.9        -1.         -1.        ]]\n",
      "\n",
      " [[-1.9         0.         -1.         -1.        ]\n",
      "  [-1.9        -1.         -1.         -1.9       ]\n",
      "  [-1.89995308 -1.89995079 -1.         -2.7098808 ]\n",
      "  [-1.89983782 -1.89937501 -1.         -2.70160228]\n",
      "  [-1.89760492 -1.89911741 -1.         -2.69114326]\n",
      "  [-1.85640941 -1.89367645 -1.         -2.62686712]\n",
      "  [-1.8740643  -1.83960967 -1.         -2.53522766]\n",
      "  [-1.         -1.80145424 -1.         -2.20046344]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.         -1.         -1.         -1.        ]]\n",
      "\n",
      " [[-1.          0.          0.         -1.        ]\n",
      "  [-1.         -1.          0.         -1.9       ]\n",
      "  [-1.         -1.          0.         -1.9       ]\n",
      "  [-1.         -1.          0.         -1.9       ]\n",
      "  [-1.         -1.          0.         -1.9       ]\n",
      "  [-1.         -1.          0.         -1.9       ]\n",
      "  [-1.         -1.          0.         -1.9       ]\n",
      "  [-1.         -1.          0.         -1.9       ]\n",
      "  [-1.         -1.          0.         -1.        ]\n",
      "  [ 0.         -1.          0.         -1.        ]]]\n",
      "Optimal Policy:\n",
      "[[1 3 3 3 3 3 3 3 3 0]\n",
      " [1 1 3 3 3 3 3 3 0 0]\n",
      " [1 1 3 3 3 3 3 3 0 0]\n",
      " [1 1 1 2 2 2 2 2 0 0]\n",
      " [1 1 2 2 2 2 2 2 0 0]\n",
      " [1 2 2 2 2 2 2 2 2 0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "environment = np.array([\n",
    "    ['#', '#', '#', '#', '#', '#', '#', '#', '#', '#'],\n",
    "    ['#', 'S', '#', '.', '.', '.', '.', '.', '.', '#'],\n",
    "    ['#', '.', '#', '.', '#', '#', '#', '#', '.', '#'],\n",
    "    ['#', '.', '.', '.', '.', '.', '.', '.', '.', '#'],\n",
    "    ['#', '#', '#', '#', '#', '#', '#', '#', 'G', '#'],\n",
    "    ['#', '#', '#', '#', '#', '#', '#', '#', '#', '#'],\n",
    "])\n",
    "\n",
    "actions = [(0, 1), (0, -1), (1, 0), (-1, 0)]\n",
    "\n",
    "num_rows, num_cols = environment.shape\n",
    "num_actions = len(actions)\n",
    "Q = np.zeros((num_rows, num_cols, num_actions))\n",
    "\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "num_episodes = 1000\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = (1, 1)  # Start state\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        epsilon = 0.1\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.choice(num_actions)\n",
    "        else:\n",
    "            action = np.argmax(Q[state[0], state[1]])\n",
    "\n",
    "        next_state = (state[0] + actions[action][0], state[1] + actions[action][1])\n",
    "\n",
    "        if next_state[0] < 0 or next_state[0] >= num_rows or next_state[1] < 0 or next_state[1] >= num_cols:\n",
    "            continue\n",
    "\n",
    "        reward = -1 \n",
    "\n",
    "        Q[state[0], state[1], action] += learning_rate * (\n",
    "            reward + discount_factor * np.max(Q[next_state[0], next_state[1]]) - Q[state[0], state[1], action]\n",
    "        )\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if environment[state[0], state[1]] == 'G':\n",
    "            done = True\n",
    "\n",
    "print(\"Learned Q-table:\")\n",
    "print(Q)\n",
    "\n",
    "optimal_policy = np.argmax(Q, axis=2)\n",
    "print(\"Optimal Policy:\")\n",
    "print(optimal_policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b9b0c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
